
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>MNIST training A Bigger Model &#8212; Papaya Jupyter Book</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Fashion MNIST IID and Balanced Dataset" href="F-MNIST.html" />
    <link rel="prev" title="MNIST Non Balanced" href="MNIST-nonbalanced.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/papaya.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Papaya Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Papaya
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MNIST Experiments
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="MNIST.html">
   MNIST Experiment: IID and Balanced Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MNIST-noniid.html">
   MNIST Non IID
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MNIST-nonbalanced.html">
   MNIST Non Balanced
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   MNIST training A Bigger Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fashion MNIST Experiments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST.html">
   Fashion MNIST IID and Balanced Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-noniid.html">
   Fashion MNIST Non IID Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-nonbalanced.html">
   Fashion MNIST Non Balanced Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-nonbalanced-bigger.html">
   Fashion MNIST Non Balanced Dataset with Bigger Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-noniid-bigger.html">
   Fashion MNIST Non IID Dataset with Bigger Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-bigger.html">
   Fashion MNIST Conv Model
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/MNIST_bigger.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/MNIST_bigger.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>MNIST training A Bigger Model</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="mnist-training-a-bigger-model">
<h1>MNIST training A Bigger Model<a class="headerlink" href="#mnist-training-a-bigger-model" title="Permalink to this headline">Â¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mnist_trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span>
                                 <span class="p">(</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                             <span class="p">]))</span>
<span class="n">mnist_testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span>
                                 <span class="p">(</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                             <span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Parameters:</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size_train</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">batch_size_test</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">500</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_trainset</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_train</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_testset</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_test</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">papayaclient</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TheModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TheModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">49</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">49</span><span class="p">,</span> <span class="mi">49</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">49</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x1</span><span class="p">)))))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clients</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batchno</span><span class="p">,</span> <span class="p">(</span><span class="n">ex_data</span><span class="p">,</span> <span class="n">ex_labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="n">clients</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">papayaclient</span><span class="o">.</span><span class="n">PapayaClient</span><span class="p">(</span><span class="n">dat</span> <span class="o">=</span> <span class="n">ex_data</span><span class="p">,</span>
                                            <span class="n">labs</span> <span class="o">=</span> <span class="n">ex_labels</span><span class="p">,</span>
                                            <span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
                                            <span class="n">num_partners</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                                            <span class="n">model_class</span> <span class="o">=</span> <span class="n">TheModel</span><span class="p">,</span>
                                            <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Train the Nodes</span>
<span class="n">num_epochs_total</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_epochs_per_swap</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_times</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_epochs_total</span> <span class="o">//</span> <span class="n">num_epochs_per_swap</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_times</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">clients</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs_per_swap</span><span class="p">):</span>
            <span class="n">n</span><span class="o">.</span><span class="n">model_train_epoch</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s1">&#39;stringy&#39;</span><span class="p">][</span><span class="n">n</span><span class="o">.</span><span class="n">epochs_trained</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_times</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">clients</span><span class="p">:</span>
            <span class="n">n</span><span class="o">.</span><span class="n">select_partners</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">clients</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="p">:</span>
                <span class="n">n</span><span class="o">.</span><span class="n">update_partner_weights</span><span class="p">()</span>
            <span class="n">n</span><span class="o">.</span><span class="n">average_partners</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>node3010epoch 0 loss 2.261340856552124
node3010epoch 1 loss 2.212891101837158
node3010epoch 2 loss 2.1522250175476074
node3010epoch 3 loss 2.0761609077453613
node3010epoch 4 loss 1.9814214706420898
node3297epoch 0 loss 2.2594196796417236
node3297epoch 1 loss 2.2021450996398926
node3297epoch 2 loss 2.1328916549682617
node3297epoch 3 loss 2.0473015308380127
node3297epoch 4 loss 1.9438883066177368
node3400epoch 0 loss 2.2872397899627686
node3400epoch 1 loss 2.246544361114502
node3400epoch 2 loss 2.1938352584838867
node3400epoch 3 loss 2.1254990100860596
node3400epoch 4 loss 2.038665533065796
node347epoch 0 loss 2.2778453826904297
node347epoch 1 loss 2.2379350662231445
node347epoch 2 loss 2.1820764541625977
node347epoch 3 loss 2.1071786880493164
node347epoch 4 loss 2.0125720500946045
node4922epoch 0 loss 2.2780885696411133
node4922epoch 1 loss 2.2403464317321777
node4922epoch 2 loss 2.1939237117767334
node4922epoch 3 loss 2.13490891456604
node4922epoch 4 loss 2.058116912841797
node2716epoch 0 loss 2.296081304550171
node2716epoch 1 loss 2.2595136165618896
node2716epoch 2 loss 2.215717077255249
node2716epoch 3 loss 2.1595373153686523
node2716epoch 4 loss 2.0880346298217773
node3010epoch 5 loss 1.8658356666564941
node3010epoch 6 loss 1.7299340963363647
node3010epoch 7 loss 1.5796592235565186
node3010epoch 8 loss 1.4241148233413696
node3010epoch 9 loss 1.273414134979248
node3297epoch 5 loss 1.8231444358825684
node3297epoch 6 loss 1.6902916431427002
node3297epoch 7 loss 1.550520658493042
node3297epoch 8 loss 1.4103115797042847
node3297epoch 9 loss 1.2786117792129517
node3400epoch 5 loss 1.9279918670654297
node3400epoch 6 loss 1.7911200523376465
node3400epoch 7 loss 1.6287888288497925
node3400epoch 8 loss 1.4537432193756104
node3400epoch 9 loss 1.2829748392105103
node347epoch 5 loss 1.8979828357696533
node347epoch 6 loss 1.765160083770752
node347epoch 7 loss 1.6199384927749634
node347epoch 8 loss 1.4697144031524658
node347epoch 9 loss 1.3220316171646118
node4922epoch 5 loss 1.9584882259368896
node4922epoch 6 loss 1.8366246223449707
node4922epoch 7 loss 1.6960347890853882
node4922epoch 8 loss 1.5449528694152832
node4922epoch 9 loss 1.3930493593215942
node2716epoch 5 loss 1.9991246461868286
node2716epoch 6 loss 1.88998281955719
node2716epoch 7 loss 1.760549783706665
node2716epoch 8 loss 1.6148231029510498
node2716epoch 9 loss 1.461326003074646
node3010epoch 10 loss 1.1372127532958984
node3010epoch 11 loss 1.0204174518585205
node3010epoch 12 loss 0.9232362508773804
node3010epoch 13 loss 0.8434005379676819
node3010epoch 14 loss 0.7778650522232056
node3297epoch 10 loss 1.1625243425369263
node3297epoch 11 loss 1.0640058517456055
node3297epoch 12 loss 0.9814983010292053
node3297epoch 13 loss 0.9122013449668884
node3297epoch 14 loss 0.8534653782844543
node3400epoch 10 loss 1.1303530931472778
node3400epoch 11 loss 1.0021023750305176
node3400epoch 12 loss 0.8979952335357666
node3400epoch 13 loss 0.8143807053565979
node3400epoch 14 loss 0.7470809817314148
node347epoch 10 loss 1.1864221096038818
node347epoch 11 loss 1.0696645975112915
node347epoch 12 loss 0.9722740054130554
node347epoch 13 loss 0.8912422060966492
node347epoch 14 loss 0.8233351111412048
node4922epoch 10 loss 1.2502461671829224
node4922epoch 11 loss 1.1239380836486816
node4922epoch 12 loss 1.016685962677002
node4922epoch 13 loss 0.9271385073661804
node4922epoch 14 loss 0.8528962731361389
node2716epoch 10 loss 1.3120722770690918
node2716epoch 11 loss 1.176906704902649
node2716epoch 12 loss 1.0607107877731323
node2716epoch 13 loss 0.9642559885978699
node2716epoch 14 loss 0.8850756287574768
node3010epoch 15 loss 1.8028624057769775
node3010epoch 16 loss 1.649907112121582
node3010epoch 17 loss 1.4980506896972656
node3010epoch 18 loss 1.3523181676864624
node3010epoch 19 loss 1.2199043035507202
node3297epoch 15 loss 1.7386679649353027
node3297epoch 16 loss 1.589866042137146
node3297epoch 17 loss 1.446777105331421
node3297epoch 18 loss 1.3150116205215454
node3297epoch 19 loss 1.1995450258255005
node3400epoch 15 loss 1.7104252576828003
node3400epoch 16 loss 1.5429701805114746
node3400epoch 17 loss 1.3828915357589722
node3400epoch 18 loss 1.2369322776794434
node3400epoch 19 loss 1.1105284690856934
node347epoch 15 loss 1.7457702159881592
node347epoch 16 loss 1.5867561101913452
node347epoch 17 loss 1.4318366050720215
node347epoch 18 loss 1.2890530824661255
node347epoch 19 loss 1.1648868322372437
node4922epoch 15 loss 1.7842981815338135
node4922epoch 16 loss 1.6326440572738647
node4922epoch 17 loss 1.4829089641571045
node4922epoch 18 loss 1.3414897918701172
node4922epoch 19 loss 1.2152624130249023
node2716epoch 15 loss 1.6798802614212036
node2716epoch 16 loss 1.5293835401535034
node2716epoch 17 loss 1.3843114376068115
node2716epoch 18 loss 1.2529706954956055
node2716epoch 19 loss 1.1397759914398193
node3010epoch 20 loss 1.6636124849319458
node3010epoch 21 loss 1.5193079710006714
node3010epoch 22 loss 1.3823894262313843
node3010epoch 23 loss 1.2569795846939087
node3010epoch 24 loss 1.1464941501617432
node3297epoch 20 loss 1.6244529485702515
node3297epoch 21 loss 1.4756382703781128
node3297epoch 22 loss 1.3414386510849
node3297epoch 23 loss 1.2245981693267822
node3297epoch 24 loss 1.125441551208496
node3400epoch 20 loss 1.65440034866333
node3400epoch 21 loss 1.5016332864761353
node3400epoch 22 loss 1.3569061756134033
node3400epoch 23 loss 1.2264267206192017
node3400epoch 24 loss 1.113134503364563
node347epoch 20 loss 1.6753730773925781
node347epoch 21 loss 1.527655839920044
node347epoch 22 loss 1.3889180421829224
node347epoch 23 loss 1.2642749547958374
node347epoch 24 loss 1.1564384698867798
node4922epoch 20 loss 1.6007148027420044
node4922epoch 21 loss 1.4521539211273193
node4922epoch 22 loss 1.317419171333313
node4922epoch 23 loss 1.2002509832382202
node4922epoch 24 loss 1.1011695861816406
node2716epoch 20 loss 1.5486385822296143
node2716epoch 21 loss 1.4042080640792847
node2716epoch 22 loss 1.2754251956939697
node2716epoch 23 loss 1.1652330160140991
node2716epoch 24 loss 1.0729820728302002
node3010epoch 25 loss 1.2573057413101196
node3010epoch 26 loss 1.1413463354110718
node3010epoch 27 loss 1.0443885326385498
node3010epoch 28 loss 0.9633758664131165
node3010epoch 29 loss 0.8956657648086548
node3297epoch 25 loss 1.1954020261764526
node3297epoch 26 loss 1.095588207244873
node3297epoch 27 loss 1.013657569885254
node3297epoch 28 loss 0.946026086807251
node3297epoch 29 loss 0.8897345066070557
node3400epoch 25 loss 1.2278484106063843
node3400epoch 26 loss 1.1131770610809326
node3400epoch 27 loss 1.0171352624893188
node3400epoch 28 loss 0.9368649125099182
node3400epoch 29 loss 0.8695970177650452
node347epoch 25 loss 1.2779247760772705
node347epoch 26 loss 1.1584750413894653
node347epoch 27 loss 1.0592025518417358
node347epoch 28 loss 0.9761638045310974
node347epoch 29 loss 0.9059925675392151
node4922epoch 25 loss 1.2119966745376587
node4922epoch 26 loss 1.1095139980316162
node4922epoch 27 loss 1.0249520540237427
node4922epoch 28 loss 0.9548074007034302
node4922epoch 29 loss 0.8961846828460693
node2716epoch 25 loss 1.2127827405929565
node2716epoch 26 loss 1.1109675168991089
node2716epoch 27 loss 1.0266045331954956
node2716epoch 28 loss 0.9566535353660583
node2716epoch 29 loss 0.898257851600647
node3010epoch 30 loss 0.8729673624038696
node3010epoch 31 loss 0.8178402185440063
node3010epoch 32 loss 0.7716905474662781
node3010epoch 33 loss 0.7326490879058838
node3010epoch 34 loss 0.6993252635002136
node3297epoch 30 loss 0.8928107023239136
node3297epoch 31 loss 0.8426644206047058
node3297epoch 32 loss 0.8005625605583191
node3297epoch 33 loss 0.7648712992668152
node3297epoch 34 loss 0.7343133091926575
node3400epoch 30 loss 0.8646717667579651
node3400epoch 31 loss 0.8078707456588745
node3400epoch 32 loss 0.7599416375160217
node3400epoch 33 loss 0.7192606329917908
node3400epoch 34 loss 0.6845066547393799
node347epoch 30 loss 0.8615901470184326
node347epoch 31 loss 0.8048065900802612
node347epoch 32 loss 0.7570335268974304
node347epoch 33 loss 0.7164077758789062
node347epoch 34 loss 0.6816262602806091
node4922epoch 30 loss 0.8905888795852661
node4922epoch 31 loss 0.8420359492301941
node4922epoch 32 loss 0.8007370233535767
node4922epoch 33 loss 0.7653397917747498
node4922epoch 34 loss 0.7347479462623596
node2716epoch 30 loss 0.9094769954681396
node2716epoch 31 loss 0.857144296169281
node2716epoch 32 loss 0.8128104209899902
node2716epoch 33 loss 0.7749841213226318
node2716epoch 34 loss 0.7424319982528687
node3010epoch 35 loss 0.6782932281494141
node3010epoch 36 loss 0.6524975299835205
node3010epoch 37 loss 0.6300486922264099
node3010epoch 38 loss 0.6102721691131592
node3010epoch 39 loss 0.5927002429962158
node3297epoch 35 loss 0.7149394750595093
node3297epoch 36 loss 0.6910984516143799
node3297epoch 37 loss 0.6702070832252502
node3297epoch 38 loss 0.6517268419265747
node3297epoch 39 loss 0.6352488994598389
node3400epoch 35 loss 0.6637901067733765
node3400epoch 36 loss 0.6367617845535278
node3400epoch 37 loss 0.6132754683494568
node3400epoch 38 loss 0.5926848649978638
node3400epoch 39 loss 0.5745160579681396
node347epoch 35 loss 0.6528715491294861
node347epoch 36 loss 0.6264049410820007
node347epoch 37 loss 0.6033167243003845
node347epoch 38 loss 0.5829694271087646
node347epoch 39 loss 0.5648893713951111
node4922epoch 35 loss 0.7202582955360413
node4922epoch 36 loss 0.6955174803733826
node4922epoch 37 loss 0.6737104654312134
node4922epoch 38 loss 0.6543371081352234
node4922epoch 39 loss 0.6369884014129639
node2716epoch 35 loss 0.7236596941947937
node2716epoch 36 loss 0.6972811222076416
node2716epoch 37 loss 0.6740574836730957
node2716epoch 38 loss 0.6534541249275208
node2716epoch 39 loss 0.6349890828132629
node3010epoch 40 loss 0.5797303318977356
node3010epoch 41 loss 0.5652229189872742
node3010epoch 42 loss 0.5521420240402222
node3010epoch 43 loss 0.5402066111564636
node3010epoch 44 loss 0.5292742252349854
node3297epoch 40 loss 0.62139892578125
node3297epoch 41 loss 0.6077228784561157
node3297epoch 42 loss 0.5952803492546082
node3297epoch 43 loss 0.5838677883148193
node3297epoch 44 loss 0.5733579993247986
node3400epoch 40 loss 0.5625431537628174
node3400epoch 41 loss 0.5477981567382812
node3400epoch 42 loss 0.5345271825790405
node3400epoch 43 loss 0.5225105881690979
node3400epoch 44 loss 0.5115529298782349
node347epoch 40 loss 0.5510610342025757
node347epoch 41 loss 0.5362668037414551
node347epoch 42 loss 0.5228994488716125
node347epoch 43 loss 0.5107234120368958
node347epoch 44 loss 0.4995608329772949
node4922epoch 40 loss 0.6250946521759033
node4922epoch 41 loss 0.6106666922569275
node4922epoch 42 loss 0.597529947757721
node4922epoch 43 loss 0.58548903465271
node4922epoch 44 loss 0.5744168758392334
node2716epoch 40 loss 0.6201127171516418
node2716epoch 41 loss 0.6048063635826111
node2716epoch 42 loss 0.5908255577087402
node2716epoch 43 loss 0.577964186668396
node2716epoch 44 loss 0.566109836101532
node3010epoch 45 loss 0.5229282975196838
node3010epoch 46 loss 0.5131794810295105
node3010epoch 47 loss 0.504250705242157
node3010epoch 48 loss 0.4959714710712433
node3010epoch 49 loss 0.4882786273956299
node3297epoch 45 loss 0.56534743309021
node3297epoch 46 loss 0.556167721748352
node3297epoch 47 loss 0.5476012229919434
node3297epoch 48 loss 0.539583683013916
node3297epoch 49 loss 0.5320438146591187
node3400epoch 45 loss 0.5057880282402039
node3400epoch 46 loss 0.4962674379348755
node3400epoch 47 loss 0.48747196793556213
node3400epoch 48 loss 0.47932717204093933
node3400epoch 49 loss 0.47175368666648865
node347epoch 45 loss 0.49179959297180176
node347epoch 46 loss 0.48211055994033813
node347epoch 47 loss 0.4731593430042267
node347epoch 48 loss 0.4648289680480957
node347epoch 49 loss 0.4570443332195282
node4922epoch 45 loss 0.5670676231384277
node4922epoch 46 loss 0.5573791265487671
node4922epoch 47 loss 0.5484054684638977
node4922epoch 48 loss 0.5400531888008118
node4922epoch 49 loss 0.5322586297988892
node2716epoch 45 loss 0.5564060211181641
node2716epoch 46 loss 0.5461010336875916
node2716epoch 47 loss 0.536489725112915
node2716epoch 48 loss 0.5275000929832458
node2716epoch 49 loss 0.5190812349319458
node3010epoch 50 loss 0.4865138828754425
node3010epoch 51 loss 0.4792253077030182
node3010epoch 52 loss 0.4725358784198761
node3010epoch 53 loss 0.46629220247268677
node3010epoch 54 loss 0.4604446589946747
node3297epoch 50 loss 0.5270625948905945
node3297epoch 51 loss 0.5201711058616638
node3297epoch 52 loss 0.5136568546295166
node3297epoch 53 loss 0.5074928998947144
node3297epoch 54 loss 0.5016236901283264
node3400epoch 50 loss 0.46822789311408997
node3400epoch 51 loss 0.4614184498786926
node3400epoch 52 loss 0.4550502598285675
node3400epoch 53 loss 0.44905751943588257
node3400epoch 54 loss 0.44342562556266785
node347epoch 50 loss 0.4524309039115906
node347epoch 51 loss 0.44529989361763
node347epoch 52 loss 0.438632071018219
node347epoch 53 loss 0.43235406279563904
node347epoch 54 loss 0.42641472816467285
node4922epoch 50 loss 0.5276942849159241
node4922epoch 51 loss 0.5205473303794861
node4922epoch 52 loss 0.5138855576515198
node4922epoch 53 loss 0.5075868368148804
node4922epoch 54 loss 0.5016076564788818
node2716epoch 50 loss 0.5128433704376221
node2716epoch 51 loss 0.5053552985191345
node2716epoch 52 loss 0.4983384907245636
node2716epoch 53 loss 0.49171608686447144
node2716epoch 54 loss 0.48545217514038086
node3010epoch 55 loss 0.45967698097229004
node3010epoch 56 loss 0.45398831367492676
node3010epoch 57 loss 0.44873180985450745
node3010epoch 58 loss 0.443807452917099
node3010epoch 59 loss 0.43918585777282715
node3297epoch 55 loss 0.498969703912735
node3297epoch 56 loss 0.4934813678264618
node3297epoch 57 loss 0.48826396465301514
node3297epoch 58 loss 0.483286589384079
node3297epoch 59 loss 0.4785173237323761
node3400epoch 55 loss 0.44188910722732544
node3400epoch 56 loss 0.4366164207458496
node3400epoch 57 loss 0.4316466152667999
node3400epoch 58 loss 0.4269312918186188
node3400epoch 59 loss 0.42245787382125854
node347epoch 55 loss 0.42329275608062744
node347epoch 56 loss 0.41772139072418213
node347epoch 57 loss 0.4124952554702759
node347epoch 58 loss 0.4075336754322052
node347epoch 59 loss 0.40281349420547485
node4922epoch 55 loss 0.4985673427581787
node4922epoch 56 loss 0.49296191334724426
node4922epoch 57 loss 0.48764777183532715
node4922epoch 58 loss 0.48259422183036804
node4922epoch 59 loss 0.47778695821762085
node2716epoch 55 loss 0.4810989201068878
node2716epoch 56 loss 0.47544896602630615
node2716epoch 57 loss 0.47008180618286133
node2716epoch 58 loss 0.46496936678886414
node2716epoch 59 loss 0.46009600162506104
node3010epoch 60 loss 0.440891832113266
node3010epoch 61 loss 0.4360579252243042
node3010epoch 62 loss 0.4316396117210388
node3010epoch 63 loss 0.42751544713974
node3010epoch 64 loss 0.4236314296722412
node3297epoch 60 loss 0.47723984718322754
node3297epoch 61 loss 0.47265636920928955
node3297epoch 62 loss 0.46827951073646545
node3297epoch 63 loss 0.4640805423259735
node3297epoch 64 loss 0.46003973484039307
node3400epoch 60 loss 0.42236053943634033
node3400epoch 61 loss 0.41803091764450073
node3400epoch 62 loss 0.4139412045478821
node3400epoch 63 loss 0.4100593328475952
node3400epoch 64 loss 0.40636590123176575
node347epoch 60 loss 0.40094712376594543
node347epoch 61 loss 0.39641448855400085
node347epoch 62 loss 0.3921487331390381
node347epoch 63 loss 0.38807860016822815
node347epoch 64 loss 0.3842078745365143
node4922epoch 60 loss 0.47580409049987793
node4922epoch 61 loss 0.47118833661079407
node4922epoch 62 loss 0.46682730317115784
node4922epoch 63 loss 0.4626408517360687
node4922epoch 64 loss 0.4586203098297119
node2716epoch 60 loss 0.45681896805763245
node2716epoch 61 loss 0.4523608088493347
node2716epoch 62 loss 0.4481068551540375
node2716epoch 63 loss 0.44402363896369934
node2716epoch 64 loss 0.44009706377983093
node3010epoch 65 loss 0.42608407139778137
node3010epoch 66 loss 0.4219352602958679
node3010epoch 67 loss 0.4181177616119385
node3010epoch 68 loss 0.41454455256462097
node3010epoch 69 loss 0.4111650884151459
node3297epoch 65 loss 0.46021944284439087
node3297epoch 66 loss 0.4562533497810364
node3297epoch 67 loss 0.4524722695350647
node3297epoch 68 loss 0.4488585293292999
node3297epoch 69 loss 0.44537588953971863
node3400epoch 65 loss 0.40729382634162903
node3400epoch 66 loss 0.403603196144104
node3400epoch 67 loss 0.4001055359840393
node3400epoch 68 loss 0.39674660563468933
node3400epoch 69 loss 0.3935340940952301
node347epoch 65 loss 0.383424311876297
node347epoch 66 loss 0.37955746054649353
node347epoch 67 loss 0.37589794397354126
node347epoch 68 loss 0.3723916709423065
node347epoch 69 loss 0.36902034282684326
node4922epoch 65 loss 0.45798009634017944
node4922epoch 66 loss 0.45402535796165466
node4922epoch 67 loss 0.45026424527168274
node4922epoch 68 loss 0.44665586948394775
node4922epoch 69 loss 0.44317254424095154
node2716epoch 65 loss 0.43759724497795105
node2716epoch 66 loss 0.4340156614780426
node2716epoch 67 loss 0.43057703971862793
node2716epoch 68 loss 0.4272518455982208
node2716epoch 69 loss 0.4240235984325409
node3010epoch 70 loss 0.41478079557418823
node3010epoch 71 loss 0.41099122166633606
node3010epoch 72 loss 0.4075406789779663
node3010epoch 73 loss 0.40430140495300293
node3010epoch 74 loss 0.40124958753585815
node3297epoch 70 loss 0.4463485777378082
node3297epoch 71 loss 0.44285455346107483
node3297epoch 72 loss 0.43950459361076355
node3297epoch 73 loss 0.43628057837486267
node3297epoch 74 loss 0.4331594407558441
node3400epoch 70 loss 0.3939233422279358
node3400epoch 71 loss 0.39075767993927
node3400epoch 72 loss 0.3877507746219635
node3400epoch 73 loss 0.38485825061798096
node3400epoch 74 loss 0.3820542097091675
node347epoch 70 loss 0.36830654740333557
node347epoch 71 loss 0.3650166690349579
node347epoch 72 loss 0.36188605427742004
node347epoch 73 loss 0.3588734269142151
node347epoch 74 loss 0.3559780418872833
node4922epoch 70 loss 0.4430088996887207
node4922epoch 71 loss 0.43957942724227905
node4922epoch 72 loss 0.4362727105617523
node4922epoch 73 loss 0.43308955430984497
node4922epoch 74 loss 0.430000901222229
node2716epoch 70 loss 0.42225512862205505
node2716epoch 71 loss 0.41922351717948914
node2716epoch 72 loss 0.4163062572479248
node2716epoch 73 loss 0.413456529378891
node2716epoch 74 loss 0.4106828272342682
node3010epoch 75 loss 0.4051809012889862
node3010epoch 76 loss 0.40170300006866455
node3010epoch 77 loss 0.3985370397567749
node3010epoch 78 loss 0.39556917548179626
node3010epoch 79 loss 0.3927585184574127
node3297epoch 75 loss 0.4348284900188446
node3297epoch 76 loss 0.4316040575504303
node3297epoch 77 loss 0.42852112650871277
node3297epoch 78 loss 0.42555397748947144
node3297epoch 79 loss 0.42269667983055115
node3400epoch 75 loss 0.38303515315055847
node3400epoch 76 loss 0.3802330195903778
node3400epoch 77 loss 0.3775711953639984
node3400epoch 78 loss 0.374994695186615
node3400epoch 79 loss 0.37248754501342773
node347epoch 75 loss 0.35551562905311584
node347epoch 76 loss 0.35262566804885864
node347epoch 77 loss 0.3498744070529938
node347epoch 78 loss 0.34721341729164124
node347epoch 79 loss 0.3446268141269684
node4922epoch 75 loss 0.43126967549324036
node4922epoch 76 loss 0.42807573080062866
node4922epoch 77 loss 0.42501235008239746
node4922epoch 78 loss 0.4220448136329651
node4922epoch 79 loss 0.4191720187664032
node2716epoch 75 loss 0.40920549631118774
node2716epoch 76 loss 0.40661394596099854
node2716epoch 77 loss 0.4041011333465576
node2716epoch 78 loss 0.4016455411911011
node2716epoch 79 loss 0.39923325181007385
node3010epoch 80 loss 0.3976091146469116
node3010epoch 81 loss 0.3942490220069885
node3010epoch 82 loss 0.3912237286567688
node3010epoch 83 loss 0.38839229941368103
node3010epoch 84 loss 0.3857012689113617
node3297epoch 80 loss 0.4244064390659332
node3297epoch 81 loss 0.42136693000793457
node3297epoch 82 loss 0.41848224401474
node3297epoch 83 loss 0.415690541267395
node3297epoch 84 loss 0.41301414370536804
node3400epoch 80 loss 0.3739517331123352
node3400epoch 81 loss 0.37134620547294617
node3400epoch 82 loss 0.368866503238678
node3400epoch 83 loss 0.3664679229259491
node3400epoch 84 loss 0.3641222417354584
node347epoch 80 loss 0.34474456310272217
node347epoch 81 loss 0.3420717120170593
node347epoch 82 loss 0.3395356237888336
node347epoch 83 loss 0.33708521723747253
node347epoch 84 loss 0.33471259474754333
node4922epoch 80 loss 0.42020153999328613
node4922epoch 81 loss 0.4172256588935852
node4922epoch 82 loss 0.41436728835105896
node4922epoch 83 loss 0.41156673431396484
node4922epoch 84 loss 0.40883439779281616
node2716epoch 80 loss 0.3980777859687805
node2716epoch 81 loss 0.3958298861980438
node2716epoch 82 loss 0.3936414420604706
node2716epoch 83 loss 0.3914715051651001
node2716epoch 84 loss 0.38933035731315613
node3010epoch 85 loss 0.3903360366821289
node3010epoch 86 loss 0.3870943784713745
node3010epoch 87 loss 0.3841780424118042
node3010epoch 88 loss 0.38146230578422546
node3010epoch 89 loss 0.3788919746875763
node3297epoch 85 loss 0.4147832989692688
node3297epoch 86 loss 0.41190987825393677
node3297epoch 87 loss 0.4091871380805969
node3297epoch 88 loss 0.40657347440719604
node3297epoch 89 loss 0.4040673077106476
node3400epoch 85 loss 0.36602941155433655
node3400epoch 86 loss 0.3635760247707367
node3400epoch 87 loss 0.36123257875442505
node3400epoch 88 loss 0.3589739203453064
node3400epoch 89 loss 0.35679441690444946
node347epoch 85 loss 0.3350068926811218
node347epoch 86 loss 0.3325958549976349
node347epoch 87 loss 0.3302856683731079
node347epoch 88 loss 0.32806769013404846
node347epoch 89 loss 0.32591062784194946
node4922epoch 85 loss 0.4105605185031891
node4922epoch 86 loss 0.4076269567012787
node4922epoch 87 loss 0.4048619866371155
node4922epoch 88 loss 0.4021751582622528
node4922epoch 89 loss 0.39955976605415344
node2716epoch 85 loss 0.38817137479782104
node2716epoch 86 loss 0.3861616849899292
node2716epoch 87 loss 0.38418468832969666
node2716epoch 88 loss 0.38222000002861023
node2716epoch 89 loss 0.38026663661003113
node3010epoch 90 loss 0.3824475407600403
node3010epoch 91 loss 0.3794749677181244
node3010epoch 92 loss 0.3767661452293396
node3010epoch 93 loss 0.37420448660850525
node3010epoch 94 loss 0.37178704142570496
node3297epoch 90 loss 0.4065454602241516
node3297epoch 91 loss 0.40376684069633484
node3297epoch 92 loss 0.4011637568473816
node3297epoch 93 loss 0.39867156744003296
node3297epoch 94 loss 0.3962955176830292
node3400epoch 90 loss 0.3596222996711731
node3400epoch 91 loss 0.35719799995422363
node3400epoch 92 loss 0.3548441231250763
node3400epoch 93 loss 0.3525869846343994
node3400epoch 94 loss 0.3504050076007843
node347epoch 90 loss 0.3261822462081909
node347epoch 91 loss 0.32395651936531067
node347epoch 92 loss 0.3218241333961487
node347epoch 93 loss 0.3197406828403473
node347epoch 94 loss 0.3177131712436676
node4922epoch 90 loss 0.4009550213813782
node4922epoch 91 loss 0.39818596839904785
node4922epoch 92 loss 0.39550840854644775
node4922epoch 93 loss 0.39292609691619873
node4922epoch 94 loss 0.3904007077217102
node2716epoch 90 loss 0.3793759047985077
node2716epoch 91 loss 0.37751150131225586
node2716epoch 92 loss 0.37566930055618286
node2716epoch 93 loss 0.3738177716732025
node2716epoch 94 loss 0.3719891905784607
node3010epoch 95 loss 0.37568309903144836
node3010epoch 96 loss 0.37283673882484436
node3010epoch 97 loss 0.37023791670799255
node3010epoch 98 loss 0.3678091764450073
node3010epoch 99 loss 0.3654802143573761
node3297epoch 95 loss 0.39983540773391724
node3297epoch 96 loss 0.3971523940563202
node3297epoch 97 loss 0.3946394622325897
node3297epoch 98 loss 0.39223986864089966
node3297epoch 99 loss 0.3899177014827728
node3400epoch 95 loss 0.3525775969028473
node3400epoch 96 loss 0.3502151668071747
node3400epoch 97 loss 0.3479762673377991
node3400epoch 98 loss 0.3457948565483093
node3400epoch 99 loss 0.3436969220638275
node347epoch 95 loss 0.31796157360076904
node347epoch 96 loss 0.3158988654613495
node347epoch 97 loss 0.31391704082489014
node347epoch 98 loss 0.31197407841682434
node347epoch 99 loss 0.3100656569004059
node4922epoch 95 loss 0.39183175563812256
node4922epoch 96 loss 0.38916313648223877
node4922epoch 97 loss 0.38660335540771484
node4922epoch 98 loss 0.3841160535812378
node4922epoch 99 loss 0.381700336933136
node2716epoch 95 loss 0.3711864948272705
node2716epoch 96 loss 0.36945849657058716
node2716epoch 97 loss 0.36773067712783813
node2716epoch 98 loss 0.36600613594055176
node2716epoch 99 loss 0.364274263381958
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">clients</span> <span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s1">&#39;stringy&#39;</span><span class="p">][</span><span class="mi">99</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>node3010epoch 99 loss 0.3654802143573761
node3297epoch 99 loss 0.3899177014827728
node3400epoch 99 loss 0.3436969220638275
node347epoch 99 loss 0.3100656569004059
node4922epoch 99 loss 0.381700336933136
node2716epoch 99 loss 0.364274263381958
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracies</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">clients</span> <span class="p">:</span>
        <span class="n">accuracies_node</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">batchno</span><span class="p">,</span> <span class="p">(</span><span class="n">ex_data</span><span class="p">,</span> <span class="n">ex_labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span> <span class="p">:</span>
            <span class="n">accuracies_node</span><span class="o">.</span><span class="n">append</span><span class="p">(((</span><span class="n">i</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">ex_data</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">ex_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">accuracies</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">node_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">accuracies_node</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracies</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{3010: 0.9030000001192093,
 3297: 0.9026999980211258,
 3400: 0.9018999993801117,
 347: 0.9,
 4922: 0.900900000333786,
 2716: 0.9022999972105026}
</pre></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="MNIST-nonbalanced.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">MNIST Non Balanced</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="F-MNIST.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fashion MNIST IID and Balanced Dataset</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Ram Mukund Kripa, Andy Zou, Ryan Jia, Kenny Huang<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>