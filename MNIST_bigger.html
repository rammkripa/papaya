
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>MNIST training A Bigger Model &#8212; Papaya Jupyter Book</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Fashion MNIST IID and Balanced Dataset" href="F-MNIST.html" />
    <link rel="prev" title="MNIST Non Balanced" href="MNIST-nonbalanced.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/papaya.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Papaya Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Papaya
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MNIST Experiments
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="MNIST.html">
   MNIST Experiment: IID and Balanced Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MNIST-noniid.html">
   MNIST Non IID
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MNIST-nonbalanced.html">
   MNIST Non Balanced
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   MNIST training A Bigger Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fashion MNIST Experiments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST.html">
   Fashion MNIST IID and Balanced Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-noniid.html">
   Fashion MNIST Non IID Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-nonbalanced.html">
   Fashion MNIST Non Balanced Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-nonbalanced-bigger.html">
   Fashion MNIST Non Balanced Dataset with Bigger Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-noniid-bigger.html">
   Fashion MNIST Non IID Dataset with Bigger Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="F-MNIST-bigger.html">
   Fashion MNIST Conv Model
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/MNIST_bigger.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/MNIST_bigger.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>MNIST training A Bigger Model</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="mnist-training-a-bigger-model">
<h1>MNIST training A Bigger Model<a class="headerlink" href="#mnist-training-a-bigger-model" title="Permalink to this headline">Â¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mnist_trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span>
                                 <span class="p">(</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                             <span class="p">]))</span>
<span class="n">mnist_testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                               <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span>
                                 <span class="p">(</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                             <span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Parameters:</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size_train</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">batch_size_test</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">500</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_trainset</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_train</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_testset</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_test</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">papayaclient</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TheModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TheModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">((</span><span class="mi">24</span> <span class="o">*</span> <span class="mi">24</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clients</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batchno</span><span class="p">,</span> <span class="p">(</span><span class="n">ex_data</span><span class="p">,</span> <span class="n">ex_labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="n">clients</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">papayaclient</span><span class="o">.</span><span class="n">PapayaClient</span><span class="p">(</span><span class="n">dat</span> <span class="o">=</span> <span class="n">ex_data</span><span class="p">,</span>
                                            <span class="n">labs</span> <span class="o">=</span> <span class="n">ex_labels</span><span class="p">,</span>
                                            <span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
                                            <span class="n">num_partners</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                                            <span class="n">model_class</span> <span class="o">=</span> <span class="n">TheModel</span><span class="p">,</span>
                                            <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Train the Nodes</span>
<span class="n">num_epochs_total</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_epochs_per_swap</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_times</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_epochs_total</span> <span class="o">//</span> <span class="n">num_epochs_per_swap</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_times</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">clients</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs_per_swap</span><span class="p">):</span>
            <span class="n">n</span><span class="o">.</span><span class="n">model_train_epoch</span><span class="p">()</span>
            <span class="c1"># print(n.logs[&#39;stringy&#39;][n.epochs_trained - 1])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_times</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">clients</span><span class="p">:</span>
            <span class="n">n</span><span class="o">.</span><span class="n">select_partners</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">clients</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="p">:</span>
                <span class="n">n</span><span class="o">.</span><span class="n">update_partner_weights</span><span class="p">()</span>
            <span class="n">n</span><span class="o">.</span><span class="n">average_partners</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>node1888epoch 0 loss 1.5461899042129517
node1888epoch 1 loss 0.807489275932312
node1888epoch 2 loss 0.6377502679824829
node1888epoch 3 loss 0.5672084093093872
node1888epoch 4 loss 0.5278582572937012
node2786epoch 0 loss 1.9848732948303223
node2786epoch 1 loss 1.3048557043075562
node2786epoch 2 loss 0.8567324876785278
node2786epoch 3 loss 0.6288896203041077
node2786epoch 4 loss 0.5104590654373169
node3408epoch 0 loss 1.9299290180206299
node3408epoch 1 loss 0.9124659895896912
node3408epoch 2 loss 0.5893670916557312
node3408epoch 3 loss 0.49376410245895386
node3408epoch 4 loss 0.4466298222541809
node3302epoch 0 loss 1.4667850732803345
node3302epoch 1 loss 0.6843005418777466
node3302epoch 2 loss 0.5147010684013367
node3302epoch 3 loss 0.44649094343185425
node3302epoch 4 loss 0.4092641770839691
node4282epoch 0 loss 1.3072646856307983
node4282epoch 1 loss 0.7020670175552368
node4282epoch 2 loss 0.5624675750732422
node4282epoch 3 loss 0.5059300065040588
node4282epoch 4 loss 0.47413772344589233
node1202epoch 0 loss 1.6222590208053589
node1202epoch 1 loss 0.6965509653091431
node1202epoch 2 loss 0.47517746686935425
node1202epoch 3 loss 0.39328184723854065
node1202epoch 4 loss 0.35003405809402466
node1888epoch 5 loss 0.5022472143173218
node1888epoch 6 loss 0.4838199019432068
node1888epoch 7 loss 0.46965277194976807
node1888epoch 8 loss 0.45827987790107727
node1888epoch 9 loss 0.44888386130332947
node2786epoch 5 loss 0.44339096546173096
node2786epoch 6 loss 0.40240636467933655
node2786epoch 7 loss 0.375591516494751
node2786epoch 8 loss 0.3569307327270508
node2786epoch 9 loss 0.3432456851005554
node3408epoch 5 loss 0.417106032371521
node3408epoch 6 loss 0.3963075876235962
node3408epoch 7 loss 0.38065433502197266
node3408epoch 8 loss 0.3683650493621826
node3408epoch 9 loss 0.3584255576133728
node3302epoch 5 loss 0.3857082724571228
node3302epoch 6 loss 0.36939752101898193
node3302epoch 7 loss 0.3573715090751648
node3302epoch 8 loss 0.34808701276779175
node3302epoch 9 loss 0.3406665623188019
node4282epoch 5 loss 0.45294925570487976
node4282epoch 6 loss 0.4374071955680847
node4282epoch 7 loss 0.42530709505081177
node4282epoch 8 loss 0.4155017137527466
node4282epoch 9 loss 0.40732434391975403
node1202epoch 5 loss 0.32317915558815
node1202epoch 6 loss 0.30493247509002686
node1202epoch 7 loss 0.29178059101104736
node1202epoch 8 loss 0.2818772494792938
node1202epoch 9 loss 0.2741549611091614
node1888epoch 10 loss 0.4409623444080353
node1888epoch 11 loss 0.4341803789138794
node1888epoch 12 loss 0.4282996952533722
node1888epoch 13 loss 0.42314353585243225
node1888epoch 14 loss 0.41857704520225525
node2786epoch 10 loss 0.3327675461769104
node2786epoch 11 loss 0.3244563341140747
node2786epoch 12 loss 0.31766989827156067
node2786epoch 13 loss 0.31199461221694946
node2786epoch 14 loss 0.3071542978286743
node3408epoch 10 loss 0.3502052426338196
node3408epoch 11 loss 0.34328603744506836
node3408epoch 12 loss 0.33737680315971375
node3408epoch 13 loss 0.33226656913757324
node3408epoch 14 loss 0.32779788970947266
node3302epoch 10 loss 0.3345753848552704
node3302epoch 11 loss 0.3294681906700134
node3302epoch 12 loss 0.32511118054389954
node3302epoch 13 loss 0.32133984565734863
node3302epoch 14 loss 0.31803518533706665
node4282epoch 10 loss 0.4003545939922333
node4282epoch 11 loss 0.39431044459342957
node4282epoch 12 loss 0.38899290561676025
node4282epoch 13 loss 0.38425713777542114
node4282epoch 14 loss 0.37999436259269714
node1202epoch 10 loss 0.26795637607574463
node1202epoch 11 loss 0.26285848021507263
node1202epoch 12 loss 0.25857892632484436
node1202epoch 13 loss 0.25492382049560547
node1202epoch 14 loss 0.2517562806606293
node1888epoch 15 loss 0.49509018659591675
node1888epoch 16 loss 0.4634162485599518
node1888epoch 17 loss 0.44758009910583496
node1888epoch 18 loss 0.437000572681427
node1888epoch 19 loss 0.42889925837516785
node2786epoch 15 loss 0.32510051131248474
node2786epoch 16 loss 0.3118307590484619
node2786epoch 17 loss 0.30360114574432373
node2786epoch 18 loss 0.29781726002693176
node2786epoch 19 loss 0.293416291475296
node3408epoch 15 loss 0.6490024328231812
node3408epoch 16 loss 0.5406443476676941
node3408epoch 17 loss 0.49067002534866333
node3408epoch 18 loss 0.4579440951347351
node3408epoch 19 loss 0.43347156047821045
node3302epoch 15 loss 0.44686228036880493
node3302epoch 16 loss 0.376604288816452
node3302epoch 17 loss 0.3498392105102539
node3302epoch 18 loss 0.3351794481277466
node3302epoch 19 loss 0.3256184756755829
node4282epoch 15 loss 0.47145289182662964
node4282epoch 16 loss 0.41765737533569336
node4282epoch 17 loss 0.3951617479324341
node4282epoch 18 loss 0.382162868976593
node4282epoch 19 loss 0.37335315346717834
node1202epoch 15 loss 0.5093368887901306
node1202epoch 16 loss 0.3888815641403198
node1202epoch 17 loss 0.3389842212200165
node1202epoch 18 loss 0.3107586205005646
node1202epoch 19 loss 0.2925325334072113
node1888epoch 20 loss 0.45341265201568604
node1888epoch 21 loss 0.4388343393802643
node1888epoch 22 loss 0.4293522536754608
node1888epoch 23 loss 0.422124981880188
node1888epoch 24 loss 0.41618505120277405
node2786epoch 20 loss 0.33227667212486267
node2786epoch 21 loss 0.3064635992050171
node2786epoch 22 loss 0.2940453886985779
node2786epoch 23 loss 0.2867438495159149
node2786epoch 24 loss 0.28193575143814087
node3408epoch 20 loss 0.47568845748901367
node3408epoch 21 loss 0.42927348613739014
node3408epoch 22 loss 0.4002789855003357
node3408epoch 23 loss 0.3798483908176422
node3408epoch 24 loss 0.36469921469688416
node3302epoch 20 loss 0.34759217500686646
node3302epoch 21 loss 0.33178290724754333
node3302epoch 22 loss 0.3221155107021332
node3302epoch 23 loss 0.3153358995914459
node3302epoch 24 loss 0.31022170186042786
node4282epoch 20 loss 0.43591442704200745
node4282epoch 21 loss 0.4000609517097473
node4282epoch 22 loss 0.38178786635398865
node4282epoch 23 loss 0.3702560365200043
node4282epoch 24 loss 0.3621349632740021
node1202epoch 20 loss 0.5307437777519226
node1202epoch 21 loss 0.4502067565917969
node1202epoch 22 loss 0.40706634521484375
node1202epoch 23 loss 0.37812310457229614
node1202epoch 24 loss 0.3566901981830597
node1888epoch 25 loss 0.4463861584663391
node1888epoch 26 loss 0.4338056445121765
node1888epoch 27 loss 0.42522209882736206
node1888epoch 28 loss 0.4185144305229187
node1888epoch 29 loss 0.41293591260910034
node2786epoch 25 loss 0.28731581568717957
node2786epoch 26 loss 0.28184637427330017
node2786epoch 27 loss 0.2781100869178772
node2786epoch 28 loss 0.2753494679927826
node2786epoch 29 loss 0.2732083201408386
node3408epoch 25 loss 0.3539830148220062
node3408epoch 26 loss 0.3434596061706543
node3408epoch 27 loss 0.33556216955184937
node3408epoch 28 loss 0.32936471700668335
node3408epoch 29 loss 0.3243376910686493
node3302epoch 25 loss 0.32053524255752563
node3302epoch 26 loss 0.31327512860298157
node3302epoch 27 loss 0.30801671743392944
node3302epoch 28 loss 0.3039668798446655
node3302epoch 29 loss 0.30071452260017395
node4282epoch 25 loss 0.37698060274124146
node4282epoch 26 loss 0.3654605746269226
node4282epoch 27 loss 0.3575025796890259
node4282epoch 28 loss 0.3515063226222992
node4282epoch 29 loss 0.3467514216899872
node1202epoch 25 loss 0.394527405500412
node1202epoch 26 loss 0.35917940735816956
node1202epoch 27 loss 0.33541062474250793
node1202epoch 28 loss 0.31790921092033386
node1202epoch 29 loss 0.30441731214523315
node1888epoch 30 loss 0.4115205407142639
node1888epoch 31 loss 0.4061899781227112
node1888epoch 32 loss 0.40220654010772705
node1888epoch 33 loss 0.3987683951854706
node1888epoch 34 loss 0.3956773281097412
node2786epoch 30 loss 0.27604377269744873
node2786epoch 31 loss 0.27343058586120605
node2786epoch 32 loss 0.27147144079208374
node2786epoch 33 loss 0.26989781856536865
node2786epoch 34 loss 0.2685846984386444
node3408epoch 30 loss 0.33101433515548706
node3408epoch 31 loss 0.32451191544532776
node3408epoch 32 loss 0.3195887506008148
node3408epoch 33 loss 0.3156214654445648
node3408epoch 34 loss 0.3123023808002472
node3302epoch 30 loss 0.3114315867424011
node3302epoch 31 loss 0.30572450160980225
node3302epoch 32 loss 0.3015368580818176
node3302epoch 33 loss 0.2982677221298218
node3302epoch 34 loss 0.2956050634384155
node4282epoch 30 loss 0.3471895158290863
node4282epoch 31 loss 0.342898428440094
node4282epoch 32 loss 0.3394930958747864
node4282epoch 33 loss 0.3365868330001831
node4282epoch 34 loss 0.3340296447277069
node1202epoch 30 loss 0.28304529190063477
node1202epoch 31 loss 0.273284375667572
node1202epoch 32 loss 0.2662597596645355
node1202epoch 33 loss 0.2608601748943329
node1202epoch 34 loss 0.25653794407844543
node1888epoch 35 loss 0.40062206983566284
node1888epoch 36 loss 0.3968391418457031
node1888epoch 37 loss 0.3938008248806
node1888epoch 38 loss 0.3910719156265259
node1888epoch 39 loss 0.3885616064071655
node2786epoch 35 loss 0.2716084122657776
node2786epoch 36 loss 0.2694401741027832
node2786epoch 37 loss 0.26783478260040283
node2786epoch 38 loss 0.2665363848209381
node2786epoch 39 loss 0.2654415965080261
node3408epoch 35 loss 0.31628111004829407
node3408epoch 36 loss 0.31186771392822266
node3408epoch 37 loss 0.3085353672504425
node3408epoch 38 loss 0.3058021664619446
node3408epoch 39 loss 0.3034597337245941
node3302epoch 35 loss 0.30227893590927124
node3302epoch 36 loss 0.2980383634567261
node3302epoch 37 loss 0.295043021440506
node3302epoch 38 loss 0.2927111089229584
node3302epoch 39 loss 0.2907745838165283
node4282epoch 35 loss 0.3329603970050812
node4282epoch 36 loss 0.3305725157260895
node4282epoch 37 loss 0.3286473751068115
node4282epoch 38 loss 0.32690247893333435
node4282epoch 39 loss 0.32526934146881104
node1202epoch 35 loss 0.2539554834365845
node1202epoch 36 loss 0.25020915269851685
node1202epoch 37 loss 0.2473328560590744
node1202epoch 38 loss 0.24497662484645844
node1202epoch 39 loss 0.2429785132408142
node1888epoch 40 loss 0.3929089307785034
node1888epoch 41 loss 0.3893277049064636
node1888epoch 42 loss 0.38663309812545776
node1888epoch 43 loss 0.38427600264549255
node1888epoch 44 loss 0.38212475180625916
node2786epoch 40 loss 0.2688120901584625
node2786epoch 41 loss 0.2668490409851074
node2786epoch 42 loss 0.2654011845588684
node2786epoch 43 loss 0.26422667503356934
node2786epoch 44 loss 0.2632324993610382
node3408epoch 40 loss 0.307399183511734
node3408epoch 41 loss 0.3038480579853058
node3408epoch 42 loss 0.3012009263038635
node3408epoch 43 loss 0.2990320920944214
node3408epoch 44 loss 0.29716479778289795
node3302epoch 40 loss 0.2994071841239929
node3302epoch 41 loss 0.29533401131629944
node3302epoch 42 loss 0.29258015751838684
node3302epoch 43 loss 0.29046425223350525
node3302epoch 44 loss 0.2887042164802551
node4282epoch 40 loss 0.32434290647506714
node4282epoch 41 loss 0.3228031098842621
node4282epoch 42 loss 0.32148975133895874
node4282epoch 43 loss 0.3202255368232727
node4282epoch 44 loss 0.31898394227027893
node1202epoch 40 loss 0.24552510678768158
node1202epoch 41 loss 0.24277664721012115
node1202epoch 42 loss 0.24071986973285675
node1202epoch 43 loss 0.23904964327812195
node1202epoch 44 loss 0.23763015866279602
node1888epoch 45 loss 0.3860745131969452
node1888epoch 46 loss 0.38304537534713745
node1888epoch 47 loss 0.3807604908943176
node1888epoch 48 loss 0.3787422776222229
node1888epoch 49 loss 0.3768891394138336
node2786epoch 45 loss 0.2666627764701843
node2786epoch 46 loss 0.2646707594394684
node2786epoch 47 loss 0.26324281096458435
node2786epoch 48 loss 0.26210924983024597
node2786epoch 49 loss 0.2611669898033142
node3408epoch 45 loss 0.3019930422306061
node3408epoch 46 loss 0.2986372113227844
node3408epoch 47 loss 0.2962087392807007
node3408epoch 48 loss 0.2942563593387604
node3408epoch 49 loss 0.29259538650512695
node3302epoch 45 loss 0.2964019179344177
node3302epoch 46 loss 0.2928466200828552
node3302epoch 47 loss 0.2904180586338043
node3302epoch 48 loss 0.28853553533554077
node3302epoch 49 loss 0.28695210814476013
node4282epoch 45 loss 0.32067227363586426
node4282epoch 46 loss 0.3188387453556061
node4282epoch 47 loss 0.31746697425842285
node4282epoch 48 loss 0.31622567772865295
node4282epoch 49 loss 0.3150428831577301
node1202epoch 45 loss 0.23997628688812256
node1202epoch 46 loss 0.2378583699464798
node1202epoch 47 loss 0.236273393034935
node1202epoch 48 loss 0.23497889935970306
node1202epoch 49 loss 0.2338719516992569
node1888epoch 50 loss 0.3799128532409668
node1888epoch 51 loss 0.37706589698791504
node1888epoch 52 loss 0.37509140372276306
node1888epoch 53 loss 0.3733851909637451
node1888epoch 54 loss 0.3718212842941284
node2786epoch 50 loss 0.2654806077480316
node2786epoch 51 loss 0.2636384069919586
node2786epoch 52 loss 0.2622569501399994
node2786epoch 53 loss 0.2611297369003296
node2786epoch 54 loss 0.26018026471138
node3408epoch 50 loss 0.2990410625934601
node3408epoch 51 loss 0.2956397235393524
node3408epoch 52 loss 0.2931853234767914
node3408epoch 53 loss 0.29123952984809875
node3408epoch 54 loss 0.28960689902305603
node3302epoch 50 loss 0.29446151852607727
node3302epoch 51 loss 0.29084327816963196
node3302epoch 52 loss 0.2884739935398102
node3302epoch 53 loss 0.286662757396698
node3302epoch 54 loss 0.2851453721523285
node4282epoch 50 loss 0.3152286410331726
node4282epoch 51 loss 0.31408581137657166
node4282epoch 52 loss 0.31309035420417786
node4282epoch 53 loss 0.3120960593223572
node4282epoch 54 loss 0.31109291315078735
node1202epoch 50 loss 0.2377670407295227
node1202epoch 51 loss 0.23553507030010223
node1202epoch 52 loss 0.2339804768562317
node1202epoch 53 loss 0.23276576399803162
node1202epoch 54 loss 0.23175452649593353
node1888epoch 55 loss 0.3775845468044281
node1888epoch 56 loss 0.3746410310268402
node1888epoch 57 loss 0.3727017641067505
node1888epoch 58 loss 0.37105098366737366
node1888epoch 59 loss 0.3695434629917145
node2786epoch 55 loss 0.26328378915786743
node2786epoch 56 loss 0.26152506470680237
node2786epoch 57 loss 0.26025059819221497
node2786epoch 58 loss 0.2592243254184723
node2786epoch 59 loss 0.25836408138275146
node3408epoch 55 loss 0.29503124952316284
node3408epoch 56 loss 0.29182296991348267
node3408epoch 57 loss 0.2895401120185852
node3408epoch 58 loss 0.2877533435821533
node3408epoch 59 loss 0.28627005219459534
node3302epoch 55 loss 0.2934997081756592
node3302epoch 56 loss 0.2897948920726776
node3302epoch 57 loss 0.2874014675617218
node3302epoch 58 loss 0.2855971157550812
node3302epoch 59 loss 0.2840995788574219
node4282epoch 55 loss 0.31343695521354675
node4282epoch 56 loss 0.3117128610610962
node4282epoch 57 loss 0.31046992540359497
node4282epoch 58 loss 0.30936354398727417
node4282epoch 59 loss 0.3083169460296631
node1202epoch 55 loss 0.2346033751964569
node1202epoch 56 loss 0.23276641964912415
node1202epoch 57 loss 0.23144122958183289
node1202epoch 58 loss 0.23038537800312042
node1202epoch 59 loss 0.22949910163879395
node1888epoch 60 loss 0.37378981709480286
node1888epoch 61 loss 0.3711438775062561
node1888epoch 62 loss 0.3692989647388458
node1888epoch 63 loss 0.3677237927913666
node1888epoch 64 loss 0.36630091071128845
node2786epoch 60 loss 0.26327335834503174
node2786epoch 61 loss 0.26146525144577026
node2786epoch 62 loss 0.2600889801979065
node2786epoch 63 loss 0.25896963477134705
node2786epoch 64 loss 0.25803637504577637
node3408epoch 60 loss 0.2922360301017761
node3408epoch 61 loss 0.28899475932121277
node3408epoch 62 loss 0.2867927551269531
node3408epoch 63 loss 0.28509727120399475
node3408epoch 64 loss 0.28369811177253723
node3302epoch 60 loss 0.291280061006546
node3302epoch 61 loss 0.28807470202445984
node3302epoch 62 loss 0.285942405462265
node3302epoch 63 loss 0.28429853916168213
node3302epoch 64 loss 0.28291112184524536
node4282epoch 60 loss 0.31053170561790466
node4282epoch 61 loss 0.3089609444141388
node4282epoch 62 loss 0.3077877163887024
node4282epoch 63 loss 0.30672886967658997
node4282epoch 64 loss 0.30572283267974854
node1202epoch 60 loss 0.23283590376377106
node1202epoch 61 loss 0.230974480509758
node1202epoch 62 loss 0.22971874475479126
node1202epoch 63 loss 0.22875696420669556
node1202epoch 64 loss 0.227966770529747
node1888epoch 65 loss 0.3718826174736023
node1888epoch 66 loss 0.36893877387046814
node1888epoch 67 loss 0.36708390712738037
node1888epoch 68 loss 0.365543931722641
node1888epoch 69 loss 0.36415907740592957
node2786epoch 65 loss 0.26155388355255127
node2786epoch 66 loss 0.25976094603538513
node2786epoch 67 loss 0.25842398405075073
node2786epoch 68 loss 0.25735852122306824
node2786epoch 69 loss 0.25648602843284607
node3408epoch 65 loss 0.2889217138290405
node3408epoch 66 loss 0.28589928150177
node3408epoch 67 loss 0.283867746591568
node3408epoch 68 loss 0.2823230028152466
node3408epoch 69 loss 0.28106069564819336
node3302epoch 65 loss 0.2912961542606354
node3302epoch 66 loss 0.28771698474884033
node3302epoch 67 loss 0.28543463349342346
node3302epoch 68 loss 0.28370901942253113
node3302epoch 69 loss 0.2822689414024353
node4282epoch 65 loss 0.307779461145401
node4282epoch 66 loss 0.30605369806289673
node4282epoch 67 loss 0.30484336614608765
node4282epoch 68 loss 0.30379390716552734
node4282epoch 69 loss 0.30281928181648254
node1202epoch 65 loss 0.2312219887971878
node1202epoch 66 loss 0.22945955395698547
node1202epoch 67 loss 0.22825635969638824
node1202epoch 68 loss 0.22733430564403534
node1202epoch 69 loss 0.22658087313175201
node1888epoch 70 loss 0.3698602616786957
node1888epoch 71 loss 0.36695578694343567
node1888epoch 72 loss 0.36512550711631775
node1888epoch 73 loss 0.36360785365104675
node1888epoch 74 loss 0.36224693059921265
node2786epoch 70 loss 0.26041802763938904
node2786epoch 71 loss 0.25863853096961975
node2786epoch 72 loss 0.2573189437389374
node2786epoch 73 loss 0.25627660751342773
node2786epoch 74 loss 0.25543123483657837
node3408epoch 70 loss 0.2887267768383026
node3408epoch 71 loss 0.28526633977890015
node3408epoch 72 loss 0.2829481363296509
node3408epoch 73 loss 0.2812167704105377
node3408epoch 74 loss 0.27983108162879944
node3302epoch 70 loss 0.28957492113113403
node3302epoch 71 loss 0.2860984206199646
node3302epoch 72 loss 0.28393855690956116
node3302epoch 73 loss 0.2823293209075928
node3302epoch 74 loss 0.28099438548088074
node4282epoch 70 loss 0.3058343827724457
node4282epoch 71 loss 0.3040216267108917
node4282epoch 72 loss 0.3027768135070801
node4282epoch 73 loss 0.3017142713069916
node4282epoch 74 loss 0.3007391393184662
node1202epoch 70 loss 0.2304108738899231
node1202epoch 71 loss 0.22859713435173035
node1202epoch 72 loss 0.2273990958929062
node1202epoch 73 loss 0.22650133073329926
node1202epoch 74 loss 0.2257779836654663
node1888epoch 75 loss 0.3667471408843994
node1888epoch 76 loss 0.36415401101112366
node1888epoch 77 loss 0.36242973804473877
node1888epoch 78 loss 0.36099937558174133
node1888epoch 79 loss 0.35973095893859863
node2786epoch 75 loss 0.26021674275398254
node2786epoch 76 loss 0.25841599702835083
node2786epoch 77 loss 0.2570604383945465
node2786epoch 78 loss 0.25598540902137756
node2786epoch 79 loss 0.2551126778125763
node3408epoch 75 loss 0.28706037998199463
node3408epoch 76 loss 0.2835080027580261
node3408epoch 77 loss 0.281212717294693
node3408epoch 78 loss 0.27952301502227783
node3408epoch 79 loss 0.2781790494918823
node3302epoch 75 loss 0.28854334354400635
node3302epoch 76 loss 0.2853190004825592
node3302epoch 77 loss 0.28324422240257263
node3302epoch 78 loss 0.28166869282722473
node3302epoch 79 loss 0.2803479731082916
node4282epoch 75 loss 0.30315548181533813
node4282epoch 76 loss 0.301567941904068
node4282epoch 77 loss 0.30042964220046997
node4282epoch 78 loss 0.29943352937698364
node4282epoch 79 loss 0.2985078692436218
node1202epoch 75 loss 0.2297283560037613
node1202epoch 76 loss 0.22785067558288574
node1202epoch 77 loss 0.2266109436750412
node1202epoch 78 loss 0.22568783164024353
node1202epoch 79 loss 0.22495107352733612
node1888epoch 80 loss 0.3648010492324829
node1888epoch 81 loss 0.36229628324508667
node1888epoch 82 loss 0.3606383204460144
node1888epoch 83 loss 0.3592694103717804
node1888epoch 84 loss 0.3580597937107086
node2786epoch 80 loss 0.25890058279037476
node2786epoch 81 loss 0.25705453753471375
node2786epoch 82 loss 0.2557581961154938
node2786epoch 83 loss 0.2547573447227478
node2786epoch 84 loss 0.25395333766937256
node3408epoch 80 loss 0.2850150167942047
node3408epoch 81 loss 0.2816333472728729
node3408epoch 82 loss 0.27946630120277405
node3408epoch 83 loss 0.27787479758262634
node3408epoch 84 loss 0.27660876512527466
node3302epoch 80 loss 0.28908485174179077
node3302epoch 81 loss 0.28551551699638367
node3302epoch 82 loss 0.2832712233066559
node3302epoch 83 loss 0.281579852104187
node3302epoch 84 loss 0.2801724076271057
node4282epoch 80 loss 0.3009029030799866
node4282epoch 81 loss 0.29928556084632874
node4282epoch 82 loss 0.2981615960597992
node4282epoch 83 loss 0.2971959412097931
node4282epoch 84 loss 0.29630640149116516
node1202epoch 80 loss 0.22859089076519012
node1202epoch 81 loss 0.226896271109581
node1202epoch 82 loss 0.22576017677783966
node1202epoch 83 loss 0.22490815818309784
node1202epoch 84 loss 0.22422590851783752
node1888epoch 85 loss 0.3643108308315277
node1888epoch 86 loss 0.3615228235721588
node1888epoch 87 loss 0.359812468290329
node1888epoch 88 loss 0.3584159314632416
node1888epoch 89 loss 0.35717684030532837
node2786epoch 85 loss 0.25901710987091064
node2786epoch 86 loss 0.2572079598903656
node2786epoch 87 loss 0.2558390498161316
node2786epoch 88 loss 0.2547614574432373
node2786epoch 89 loss 0.2538954019546509
node3408epoch 85 loss 0.283206969499588
node3408epoch 86 loss 0.279865562915802
node3408epoch 87 loss 0.2777383327484131
node3408epoch 88 loss 0.27619749307632446
node3408epoch 89 loss 0.2749873697757721
node3302epoch 85 loss 0.2886727452278137
node3302epoch 86 loss 0.2849300801753998
node3302epoch 87 loss 0.28264203667640686
node3302epoch 88 loss 0.2809436321258545
node3302epoch 89 loss 0.27954351902008057
node4282epoch 85 loss 0.29818835854530334
node4282epoch 86 loss 0.2969956398010254
node4282epoch 87 loss 0.2960343062877655
node4282epoch 88 loss 0.29514315724372864
node4282epoch 89 loss 0.2942929267883301
node1202epoch 85 loss 0.22810031473636627
node1202epoch 86 loss 0.2263496071100235
node1202epoch 87 loss 0.22518585622310638
node1202epoch 88 loss 0.2243177890777588
node1202epoch 89 loss 0.22362712025642395
node1888epoch 90 loss 0.3631635308265686
node1888epoch 91 loss 0.360343337059021
node1888epoch 92 loss 0.3586117625236511
node1888epoch 93 loss 0.35720276832580566
node1888epoch 94 loss 0.35595858097076416
node2786epoch 90 loss 0.2572856545448303
node2786epoch 91 loss 0.255623996257782
node2786epoch 92 loss 0.2544323801994324
node2786epoch 93 loss 0.2534979581832886
node2786epoch 94 loss 0.2527404725551605
node3408epoch 90 loss 0.28147342801094055
node3408epoch 91 loss 0.27832770347595215
node3408epoch 92 loss 0.27628231048583984
node3408epoch 93 loss 0.27479663491249084
node3408epoch 94 loss 0.27363330125808716
node3302epoch 90 loss 0.28856897354125977
node3302epoch 91 loss 0.2848595976829529
node3302epoch 92 loss 0.2825697064399719
node3302epoch 93 loss 0.2808803617954254
node3302epoch 94 loss 0.279491662979126
node4282epoch 90 loss 0.2964078485965729
node4282epoch 91 loss 0.2950185537338257
node4282epoch 92 loss 0.29401424527168274
node4282epoch 93 loss 0.2931325435638428
node4282epoch 94 loss 0.292310893535614
node1202epoch 90 loss 0.2268470674753189
node1202epoch 91 loss 0.22527539730072021
node1202epoch 92 loss 0.2242085039615631
node1202epoch 93 loss 0.2234015017747879
node1202epoch 94 loss 0.22275537252426147
node1888epoch 95 loss 0.3608856499195099
node1888epoch 96 loss 0.35831716656684875
node1888epoch 97 loss 0.3566399812698364
node1888epoch 98 loss 0.3552776277065277
node1888epoch 99 loss 0.35409218072891235
node2786epoch 95 loss 0.25813233852386475
node2786epoch 96 loss 0.25624752044677734
node2786epoch 97 loss 0.2548363506793976
node2786epoch 98 loss 0.25373220443725586
node2786epoch 99 loss 0.2528506815433502
node3408epoch 95 loss 0.2796116769313812
node3408epoch 96 loss 0.2765235900878906
node3408epoch 97 loss 0.2745947241783142
node3408epoch 98 loss 0.2732130289077759
node3408epoch 99 loss 0.2721341550350189
node3302epoch 95 loss 0.289368599653244
node3302epoch 96 loss 0.28559592366218567
node3302epoch 97 loss 0.28321489691734314
node3302epoch 98 loss 0.2814217209815979
node3302epoch 99 loss 0.27993401885032654
node4282epoch 95 loss 0.29470372200012207
node4282epoch 96 loss 0.29350903630256653
node4282epoch 97 loss 0.29255375266075134
node4282epoch 98 loss 0.2916756868362427
node4282epoch 99 loss 0.29084330797195435
node1202epoch 95 loss 0.22645430266857147
node1202epoch 96 loss 0.2247922420501709
node1202epoch 97 loss 0.22368121147155762
node1202epoch 98 loss 0.222855344414711
node1202epoch 99 loss 0.22220300137996674
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">clients</span> <span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s1">&#39;stringy&#39;</span><span class="p">][</span><span class="mi">99</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>node1888epoch 99 loss 0.35409218072891235
node2786epoch 99 loss 0.2528506815433502
node3408epoch 99 loss 0.2721341550350189
node3302epoch 99 loss 0.27993401885032654
node4282epoch 99 loss 0.29084330797195435
node1202epoch 99 loss 0.22220300137996674
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracies</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">clients</span> <span class="p">:</span>
        <span class="n">accuracies_node</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">batchno</span><span class="p">,</span> <span class="p">(</span><span class="n">ex_data</span><span class="p">,</span> <span class="n">ex_labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span> <span class="p">:</span>
            <span class="n">accuracies_node</span><span class="o">.</span><span class="n">append</span><span class="p">(((</span><span class="n">i</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">ex_data</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">ex_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">accuracies</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">node_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">accuracies_node</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracies</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1888: 0.9180999994277954,
 2786: 0.9181000024080277,
 3408: 0.9206999987363815,
 3302: 0.9205000013113022,
 4282: 0.9197999954223632,
 1202: 0.918299999833107}
</pre></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="MNIST-nonbalanced.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">MNIST Non Balanced</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="F-MNIST.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fashion MNIST IID and Balanced Dataset</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Ram Mukund Kripa, Andy Zou, Ryan Jia, Kenny Huang<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>